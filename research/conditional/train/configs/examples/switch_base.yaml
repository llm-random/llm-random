parent: research/conditional/train/configs/baselines/gpt/dense/base.yaml
md5_parent_hash: 17e23fa8efdf44ed2a84b8e15941039c
interactive_debug: false
n_gpus: 4
time: "1:00:00"
hf_datasets_cache: /local_storage_1/dataset_cache
singularity_image: /home/jkrajewski_a100/images/sparsity_2023.11.10_15.23.19.sif

params:

    n_steps: 100
    n_blocks: 16

    #tuning
    ^learning_rate: [2e-4]
    weight_decay: 0.1


    #name
    name: switch_base16l_test
    tags: [switch,base16l]

    #switch specific
    load_balancing_loss_weight: 0.01
    capacity_factor: 1
    dont_vectorize_switch: false
    
    # standard
    ff_mode: token_choice
    init_type: truncated_normal
    init_scale: 0.3

    # moe specific
    # N_experts: 32 every expert size of normal DFF in vanilla
    n_experts: 32
    expert_size: 3072

    #data
    ^batch_size: [4096]
    dataset_type: c4
    cutoff: 1024
    
    #eval and logging
    log_gradients_and_weights: false
    decoding_interval: 0



    #fsdp
    fsdp_enabled: true
    fsdp_modules_to_wrap: "TransformerBlock,EmbeddingLayer,PredictionHead"
    fsdp_selective_precision_modules: ["AttentionMechanism,TokenChoiceRouter"]
    # activation_checkpointing_modules: ["EmbeddingLayer,TransformerBlock,PredictionHead"]



    #throughput
    mixed_precision: true
    mixed_precision_dtype: bfloat16
    flash_attention: false
    gradient_accumulation_steps: 1
    loss_checkpoint_chungs: 8