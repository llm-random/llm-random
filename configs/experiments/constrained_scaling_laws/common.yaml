parent: configs/baselines/gpt/expert_choice/granularity/4/base.yaml
md5_parent_hash: 6cd6b241d09b039d86d8c51b5f0b4a01
time: "160:00:00"
interactive_debug_session: false
interactive_debug: false
n_gpus: 1

params:
  name: constrained_scaling_grid_fixed
  tags: ["constrained_scaling_grid"]
  ff_mode: token_choice
  capacity_factor: 1.0
  activation_type: silu
  moe_inner_expert: ff_gated
  expansion_rate: 8
  granularity: 1
  get_router_values_from: weights
  layer_norm_in_expert_choice: False

  n_steps: 10 # mock value for tests
  # [1907, 3814, 7628, 15256, 30512]
  # 500M, 1B, 2B, 4B, 8B

  dmodel: 256
  n_blocks: 4
  n_att_heads: 4

  batch_size: 512
  cutoff: 512
  final_lr_step: -1
  lr_warmup_percent: 0.01
  lr_warmup_steps: 0
  final_lr_fraction: 0.1
  init_scale: 0.1
  learning_rate: 0
  weight_decay: 0.1
  zloss_weight: 0.001
  load_balancing_loss_weight: 0.01
  data_seed: 42

  save_weights_interval: 0
  mixed_precision: True
  mixed_precision_dtype: bfloat16
  flash_attention: true
  loss_checkpoint_chungs: 0

  effective_dff_x: 3
  grad_clip: 0.1
  attention_mode: "rope"
  no_positional_embedding: false
  lr_trapezoidal_decay_percent: 0.2
  scheduler: trapezoidal

  fsdp_enabled: true
  fsdp_modules_to_wrap: "EmbeddingLayer,PredictionHead,TransformerBlock"
  activation_checkpointing_modules: "EmbeddingLayer,PredictionHead,TransformerBlock"
  fsdp_selective_precision_modules: "AttentionMechanism,MoeGating,RoPE"

