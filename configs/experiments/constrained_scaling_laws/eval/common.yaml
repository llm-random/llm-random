runner: "research.conditional.train.cc_train"
time: "00-12:00:00"
interactive_debug_session: false
interactive_debug: false
cpus_per_gpu: 16
n_gpus: 1

params:
  name: constrained_scaling_grid_21_11
  tags: ["constrained_scaling_grid"]

  # architecture
  ff_mode: token_choice
  capacity_factor: 1.0
  activation_type: silu
  moe_inner_expert: ff_gated
  granularity: 1
  effective_dff_x: 3
  attention_mode: "rope"
  no_positional_embedding: true
  model_type: "gpt"

  # hyperparameters
  init_type: "truncated_normal"
  init_scale: 0.1
  weight_decay: 0.1
  zloss_weight: 0.001
  load_balancing_loss_weight: 0.01
  grad_clip: 0.1
  dataset_type: "fineweb-edu"

  # learning rate
  learning_rate: 0
  use_lr_scaling: true
  lr_warmup_tokens: 0.13 # 130M
  # lr_warmup_steps: 2000 # 130M
  lr_trapezoidal_decay_fraction: 0.2
  lr_trapezoidal_decay_fraction_unit: "tokens"
  scheduler: trapezoidal

  # batch size
  batch_size: 512
  cutoff: 512
  # batch_size_rampup_transition_points: [0.5, 1.0]
  # batch_size_rampup_sizes: [128, 256]

  # eval
  final_eval_seed: 420
  final_eval_dataloader_batch_size: 64
  n_final_eval_batches: 2048
  run_final_eval: true
  # n_final_eval_batches: 0
  eval_interval: -1

  # repeater
  # 500M, 1B, 2B, 4B, 8B, 16B, 32B, 48B, 64B, 80B, 96B, 112B, 128B
  # do tego trzeba dodac rampup
  # [1923, 3830, 7645, 15274, 30533, 61051, 122086, 183121, 244156, 305191, 366226, 427262, 488297]
  save_weights_interval: 0
  n_tokens: 128
  # n_steps: 488297
  save_weights_path: "./constrained_scaling_laws_grid"
  checkpoint_manager: false
  # scheduler_trapezoidal_slides: "[{'n_tokens':0.5},{'n_tokens':1},{'n_tokens':2},{'n_tokens':4},{'n_tokens':8},{'n_tokens':16},{'n_tokens':32},{'n_tokens':48},{'n_tokens':64},{'n_tokens':80},{'n_tokens':96},{'n_tokens':112}]"
  # scheduler_trapezoidal_slides: "[{'n_steps':1923},{'n_steps':3830},{'n_steps':7645},{'n_steps':15274},{'n_steps':30533},{'n_steps':61051},{'n_steps':122086},{'n_steps':183121},{'n_steps':244156},{'n_steps':305191},{'n_steps':366226},{'n_steps':427262}]"
  ^final_eval_step: [495910, 434875, 373840, 312805, 251769, 190734, 129699, 68664, 38146, 22888, 15258, 11443, 7629]


  # precision and distributed
  mixed_precision: True
  mixed_precision_dtype: bfloat16
  flash_attention: True
  loss_checkpoint_chungs: 0

  fsdp_enabled: True
  fsdp_modules_to_wrap: "EmbeddingLayer,PredictionHead,TransformerBlock"
  activation_checkpointing_modules: "EmbeddingLayer,PredictionHead,TransformerBlock"
  fsdp_selective_precision_modules: "AttentionMechanism,MoeGating,RoPE"

  logger_types: "neptune"
  project_name: "pmtest/llm-random"
  logging_interval_heavy: 50000
  num_workers: 4