time: "5-00:00:00"
interactive_debug_session: false
interactive_debug: false
n_gpus: 4
cpus_per_gpu: 16

params:
  name: "large_dense_baseline"
  tags: ["relativity_paper", "large_model", "dense", "baseline", "std"]

  # grid
  # ^data_seed: [2167]
  ^data_seed: [2170, 10065]

  # training
  n_steps: 50000
  final_lr_step: 50000
  lr_warmup_steps: 500
  batch_size: 384
  cutoff: 1024

  # model
  model_type: gpt
  dmodel: 1536
  dff: 6144
  n_att_heads: 12
  n_blocks: 24
  ff_mode: swi_glu

  # learning
  scheduler: cosine
  learning_rate: 5e-4
  final_lr_fraction: 0.066666
  grad_clip: 0.1
  weight_decay: 0.15
  init_scale: 0.333
  init_type: truncated_normal

  fsdp_enabled: true
  fsdp_modules_to_wrap: EmbeddingLayer,PredictionHead,TransformerBlock
  fsdp_selective_precision_modules: AttentionMechanism
  mixed_precision: true
  mixed_precision_dtype: bfloat16

  # other
  dataset_type: c4
  flash_attention: true
  loss_checkpoint_chungs: 0
  print_parameter_names: true
  activation_checkpointing_modules: EmbeddingLayer,PredictionHead,TransformerBlock
  
  # logging
  logger_types: "neptune"
  project_name: "pmtest/llm-random"
  logging_interval_heavy: 5000
  logging_interval_loss: 1000
  save_weights_interval: -1