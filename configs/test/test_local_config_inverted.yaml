parent: configs/baselines/gpt/dense/medium.yaml
md5_parent_hash: 1706dd43c527be74a6282b0319e9c31e
time: 0-12:00:00
# cuda_visible: "2"
interactive_debug: true
runner: "research.inverted.train.train"
argparse: "research.inverted.utils.argparse"

# LOCAL:
n_gpus: 1

params:
  # tokenizer: gpt

  # blanx args
  name: "inverted_transformer"
  mixed_precision: true
  mixed_precision_dtype: bfloat16
  flash_attention: false
#  logger_types: ["stdout"]
  # use_neptune: false
  grad_clip: 1.0


  n_steps: 10000
  ^learning_rate: [1e-2, 1e-3, 1e-4]
  decoding_interval: 0
  logging_interval_heavy: 50
  # eval_interval: 1000
  init_type: truncated_normal
  init_scale: 0.1
  ^inverted: [true, false]
  final_lr_step: 10000
  lr_warmup_steps: 200 # 1-5%
  loss_checkpoint_chungs: 4


  # use_hidden_weights_fanin: true
  # output_weight_relative_init_scale: 0.1
  # output_weight_multiplier: 1.0

  # loss_checkpoint_chungs: 2

  # mamba_mode: 'recursive'
  # mamba_n_levels: 2
  # block_modules: ["mamba"]

  # LOCAL:
  dataset_type: c4
  batch_size: 256
  # train_dataset_path: "train_dataset_path"
  # validation_dataset_path: "aaa/ggg"

  use_dummy_dataset: true
  num_workers: 0
  save_weights_interval: 0
